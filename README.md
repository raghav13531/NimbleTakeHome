# NimbleTakeHome
Take Home Assessment (Video Rendering WebApp Using WebTransport and WebRTC

DESIGN CHOICES AND OVERALL THOUGHTS FOR THE PROJECT

When I started this project, I had very little experience with WebRTC or WebTransport, just some basic server-client work from past projects. Diving into real-time communication was pretty overwhelming, and I relied heavily on documentation and trial-and-error. At first, I only knew simple socket programming or HTTP-based setups, so WebRTC and WebTransport felt completely foreign. I spent a lot of time going through the documentation that was available online and I tried to find resources that used either of the two technologies -- I ended up finding a Google tutorial on WebTransport which had code for both server and client and allowed for the client to send information back to the server. This really helped me get started, and I sort of used this as starter code. 

One of the biggest struggles was dealing with SSL certificates, which WebTransport and WebRTC require. I didn’t understand how to generate them initially. Using OpenSSL commands led to constant errors—invalid key formats, certificate chain issues, and browsers rejecting my self-signed certificates. I was following instructions for creating the certificates online and my server was continously unreachable due to rejected security certificates, but after a lot of debugging, I realized that adding the certificates to my trusted root authentication on my computer wasn't getting them added to chrome. I uploaded the certificate directly to Chrome and the server was reachable from that point on.

I tried using WebRTC for the video streaming because of the requirement, but setting up peer connections, handling codecs, and making it work with my Python server that already was integrated with WebTransport was too complicated. After hours of debugging, I gave up and switched to just using WebTransport to send the JPEG video frames (not H.264 enocded video frames), which was simpler and easier to understand. I streamed raw JPEG frames over WebTransport using OpenCV on the server and JavaScript on the client to render them on a canvas. This got a semi-working project, and although I didn't meet the requirement with WebRTC, I thought that getting something working was more practical. There is a lot of latecny at the moment and the frames sent from the server are sort of backlogged -- the ball is stationary for some amount of time and then all the video frames are sent to the client at the same time and the ball's movement is updated. 

I wasn't able to finish the entirety of the project -- I didn't have enough time to figure out the coordinates of the ball and send it back to the server to check the latency of the connection. That is something I think I could finish this given an hour or two more (prior committments and university coursework took up some of my other time over the three days allotted for the hackathon).

The final setup has WebTransport for streaming, a Python server with aioquic for QUIC/WebTransport support, and a browser client using canvas. This didn't follow what was asked in the guideliIt wasn’t as advanced as I originally planned, but it worked, and I learned a lot about real-time protocols along the way.
